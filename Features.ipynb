{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (accuracy_score, classification_report, \n",
    "                             precision_recall_curve, confusion_matrix, \n",
    "                             roc_auc_score)\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def calc_haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Compute the Haversine distance between two latitude/longitude points.\n",
    "    Returns the distance in kilometers.\n",
    "    \"\"\"\n",
    "    # Convert from degrees to radians\n",
    "    rad_lat1, rad_lon1, rad_lat2, rad_lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    \n",
    "    # Haversine formula\n",
    "    dlat = rad_lat2 - rad_lat1\n",
    "    dlon = rad_lon2 - rad_lon1\n",
    "    a = np.sin(dlat / 2)**2 + np.cos(rad_lat1) * np.cos(rad_lat2) * np.sin(dlon / 2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    radius_earth = 6371\n",
    "    return c * radius_earth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training dataset\n",
    "train = pd.read_csv('train.csv')\n",
    "\n",
    "# Copy and sort by time\n",
    "df = train.copy()\n",
    "df.sort_values(by='unix_time', inplace=True)\n",
    "\n",
    "# Group by credit card number\n",
    "df_by_cc = df.groupby('cc_num')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0d/79zy7nxj1pl2rsqrgy7_wqrw0000gn/T/ipykernel_42926/240637076.py:14: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['time_between_last_transaction'].fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Calculate distance features\n",
    "df['distance'] = calc_haversine_distance(df['lat'], df['long'], df['merch_lat'], df['merch_long'])\n",
    "\n",
    "# Category-based features\n",
    "df['prev_category_same'] = df_by_cc['category'].shift(-1) == df['category']\n",
    "df['next_category_same'] = df_by_cc['category'].shift(1) == df['category']\n",
    "df['both_category_different'] = df['prev_category_same'] | df['next_category_same']\n",
    "\n",
    "# Time-based features\n",
    "df['avg_time'] = df.groupby(['cc_num', 'category'])['unix_time'].transform('mean')\n",
    "df['cat_time_diff'] = df['unix_time'] - df['avg_time']\n",
    "df['avg_time_cc'] = df.groupby('cc_num')['unix_time'].diff().mean()\n",
    "df['time_between_last_transaction'] = df['unix_time'] - df['unix_time'].shift(1)\n",
    "df['time_between_last_transaction'].fillna(0, inplace=True)\n",
    "df['diff_time'] = df['avg_time_cc'] - df['time_between_last_transaction']\n",
    "\n",
    "# Amount-based features\n",
    "df['log_amt'] = np.log1p(df['amt'])\n",
    "df['counts'] = df.groupby('cc_num')['cc_num'].transform('count')\n",
    "df['counts_per_cc_category'] = df.groupby(['cc_num', 'category'])['category'].transform('count')\n",
    "\n",
    "# Rolling means\n",
    "df['ma_10'] = df.groupby('cc_num')['amt'].transform(lambda s: s.rolling(window=10).mean()).fillna(df['amt'])\n",
    "df['ma_3'] = df.groupby('cc_num')['amt'].transform(lambda s: s.rolling(window=3).mean()).fillna(df['amt'])\n",
    "\n",
    "df['max_time_ma_3'] = df.groupby('cc_num')['ma_3'].transform('max')\n",
    "df['avg_money_per_category'] = df.groupby('category')['amt'].transform('mean')\n",
    "\n",
    "df['prev_amt_diff'] = df_by_cc['amt'].diff().fillna(0)\n",
    "df['next_amt_diff'] = df_by_cc['amt'].diff(-1).fillna(0)\n",
    "\n",
    "# High amount flags\n",
    "high_amt_thresh = df['amt'].quantile(0.9)\n",
    "df['high_amt_flag'] = (df['amt'] > high_amt_thresh).astype(int)\n",
    "df['large_txn_ratio'] = df.groupby('cc_num')['high_amt_flag'].transform('mean')\n",
    "\n",
    "df['Hour'] = pd.to_timedelta(df['trans_time']).dt.total_seconds() / 3600\n",
    "\n",
    "# Max unix time for ma_3\n",
    "max_unix_times = df.loc[df.groupby('cc_num')['ma_3'].idxmax(), ['cc_num', 'unix_time']]\n",
    "df['max_unix_time'] = df['cc_num'].map(max_unix_times.set_index('cc_num')['unix_time'])\n",
    "\n",
    "df['v5'] = (df['unix_time'] - df['max_unix_time']).abs()\n",
    "df['v6'] = df['amt'] / (df['v5'] + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'is_fraud'\n",
    "\n",
    "drop_columns = [\n",
    "    target_column, 'id', 'zip', 'avg_time_cc', 'state', 'long', 'lat', 'merch_lat', 'merch_long', \n",
    "    'first', 'last', 'street', 'city', 'dob', 'merchant', 'job', 'trans_num', 'gender',\n",
    "    'time_between_last_transaction', 'cc_num', 'city_pop', 'counts_per_cc_category', \n",
    "    'diff_time', 'avg_time', 'trans_date', 'ma_10', 'amt', 'next_category_same', \n",
    "    'distance', 'trans_time', 'max_unix_time'\n",
    "]\n",
    "\n",
    "X = df.drop(columns=drop_columns)\n",
    "y = df[target_column]\n",
    "\n",
    "# Label encode any non-numeric columns\n",
    "non_numeric_cols = X.select_dtypes(include=['object']).columns\n",
    "encoder = LabelEncoder()\n",
    "for col in non_numeric_cols:\n",
    "    X[col] = encoder.fit_transform(X[col].astype(str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.9911385063595042\n",
      "Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00     65702\n",
      "           1       0.98      0.94      0.96      8439\n",
      "\n",
      "    accuracy                           0.99     74141\n",
      "   macro avg       0.98      0.97      0.98     74141\n",
      "weighted avg       0.99      0.99      0.99     74141\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,                 \n",
    "    class_weight='balanced', \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "rf_accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Feature  Importance\n",
      "5                   log_amt    0.193916\n",
      "15                       v5    0.170388\n",
      "16                       v6    0.154542\n",
      "12            high_amt_flag    0.083539\n",
      "4             cat_time_diff    0.060992\n",
      "14                     Hour    0.053917\n",
      "3   both_category_different    0.049851\n",
      "7                      ma_3    0.044628\n",
      "11            next_amt_diff    0.042899\n",
      "10            prev_amt_diff    0.035173\n",
      "0                 unix_time    0.031847\n",
      "2        prev_category_same    0.016946\n",
      "13          large_txn_ratio    0.014489\n",
      "9    avg_money_per_category    0.013958\n",
      "8             max_time_ma_3    0.012925\n",
      "6                    counts    0.011177\n",
      "1                  category    0.008813\n"
     ]
    }
   ],
   "source": [
    "feature_importances = rf_model.feature_importances_\n",
    "importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})\n",
    "print(importance_df.sort_values(by='Importance', ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     65702\n",
      "           1       0.88      0.98      0.93      8439\n",
      "\n",
      "    accuracy                           0.98     74141\n",
      "   macro avg       0.94      0.98      0.96     74141\n",
      "weighted avg       0.98      0.98      0.98     74141\n",
      "\n",
      "XGBoost Accuracy: 0.9824523542978919\n"
     ]
    }
   ],
   "source": [
    "class_0_count, class_1_count = np.bincount(y_train)\n",
    "scale_pos_weight = class_0_count / class_1_count\n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,       \n",
    "    max_depth=4,             \n",
    "    min_child_weight=5,      \n",
    "    gamma=0.2,                \n",
    "    subsample=0.8,            \n",
    "    colsample_bytree=0.8,    \n",
    "    scale_pos_weight=scale_pos_weight, \n",
    "    random_state=43\n",
    ")\n",
    "\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "print(\"XGBoost Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "print(\"XGBoost Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Threshold: 0.42\n",
      "Classification Report with Adjusted Threshold:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00     65702\n",
      "           1       0.97      0.96      0.96      8439\n",
      "\n",
      "    accuracy                           0.99     74141\n",
      "   macro avg       0.98      0.98      0.98     74141\n",
      "weighted avg       0.99      0.99      0.99     74141\n",
      "\n",
      "Accuracy with Adjusted Threshold: 0.9918533604887984\n"
     ]
    }
   ],
   "source": [
    "y_probs_rf = rf_model.predict_proba(X_test)[:, 1]\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_probs_rf)\n",
    "\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "optimal_idx = np.argmax(f1_scores)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "print(\"Optimal Threshold:\", optimal_threshold)\n",
    "\n",
    "y_pred_adj = (y_probs_rf >= optimal_threshold).astype(int)\n",
    "print(\"Classification Report with Adjusted Threshold:\")\n",
    "print(classification_report(y_test, y_pred_adj))\n",
    "print(\"Accuracy with Adjusted Threshold:\", accuracy_score(y_test, y_pred_adj))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0d/79zy7nxj1pl2rsqrgy7_wqrw0000gn/T/ipykernel_42926/2921812368.py:17: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  combined['time_between_last_transaction'].fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "final_train = pd.read_csv('train.csv')\n",
    "final_test = pd.read_csv('test.csv')\n",
    "combined = pd.concat([final_train, final_test], axis=0)\n",
    "combined.sort_values(by='unix_time', inplace=True)\n",
    "combined_by_cc = combined.groupby('cc_num')\n",
    "\n",
    "# Repeat feature engineering for combined data\n",
    "combined['distance'] = calc_haversine_distance(combined['lat'], combined['long'], combined['merch_lat'], combined['merch_long'])\n",
    "combined['prev_category_same'] = combined_by_cc['category'].shift(-1) == combined['category']\n",
    "combined['next_category_same'] = combined_by_cc['category'].shift(1) == combined['category']\n",
    "combined['both_category_different'] = combined['prev_category_same'] | combined['next_category_same']\n",
    "\n",
    "combined['avg_time'] = combined.groupby(['cc_num', 'category'])['unix_time'].transform('mean')\n",
    "combined['cat_time_diff'] = combined['unix_time'] - combined['avg_time']\n",
    "combined['avg_time_cc'] = combined.groupby('cc_num')['unix_time'].diff().mean()\n",
    "combined['time_between_last_transaction'] = combined['unix_time'] - combined['unix_time'].shift(1)\n",
    "combined['time_between_last_transaction'].fillna(0, inplace=True)\n",
    "combined['diff_time'] = combined['avg_time_cc'] - combined['time_between_last_transaction']\n",
    "\n",
    "combined['log_amt'] = np.log1p(combined['amt'])\n",
    "combined['counts'] = combined.groupby('cc_num')['cc_num'].transform('count')\n",
    "combined['counts_per_cc_category'] = combined.groupby(['cc_num', 'category'])['category'].transform('count')\n",
    "\n",
    "combined['ma_10'] = combined.groupby('cc_num')['amt'].transform(lambda s: s.rolling(window=10).mean()).fillna(combined['amt'])\n",
    "combined['ma_3'] = combined.groupby('cc_num')['amt'].transform(lambda s: s.rolling(window=3).mean()).fillna(combined['amt'])\n",
    "\n",
    "combined['max_time_ma_3'] = combined.groupby('cc_num')['ma_3'].transform('max')\n",
    "combined['avg_money_per_category'] = combined.groupby('category')['amt'].transform('mean')\n",
    "\n",
    "combined['prev_amt_diff'] = combined_by_cc['amt'].diff().fillna(0)\n",
    "combined['next_amt_diff'] = combined_by_cc['amt'].diff(-1).fillna(0)\n",
    "\n",
    "high_amt_thresh = combined['amt'].quantile(0.9)\n",
    "combined['high_amt_flag'] = (combined['amt'] > high_amt_thresh).astype(int)\n",
    "combined['large_txn_ratio'] = combined.groupby('cc_num')['high_amt_flag'].transform('mean')\n",
    "\n",
    "combined['Hour'] = pd.to_timedelta(combined['trans_time']).dt.total_seconds() / 3600\n",
    "\n",
    "max_unix_times_combined = combined.loc[combined.groupby('cc_num')['ma_3'].idxmax(), ['cc_num', 'unix_time']]\n",
    "max_unix_times_combined = max_unix_times_combined.drop_duplicates(subset='cc_num').set_index('cc_num')\n",
    "combined['max_unix_time'] = combined['cc_num'].map(max_unix_times_combined['unix_time'])\n",
    "\n",
    "combined['v5'] = abs(combined['unix_time'] - combined['max_unix_time'])\n",
    "combined['v6'] = combined['amt'] / (combined['v5'] + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created: submission.csv\n"
     ]
    }
   ],
   "source": [
    "final = combined[combined['id'].isin(final_test['id'])]\n",
    "\n",
    "final_drop_cols = [\n",
    "    'is_fraud', 'id', 'zip', 'avg_time_cc', 'state', 'long', 'lat', 'merch_lat', 'merch_long',\n",
    "    'first', 'last', 'street', 'city', 'dob', 'merchant', 'job', 'trans_num', 'gender', \n",
    "    'time_between_last_transaction', 'cc_num', 'city_pop', 'counts_per_cc_category', 'diff_time',\n",
    "    'avg_time', 'trans_date', 'ma_10', 'amt', 'next_category_same', 'distance', 'trans_time', \n",
    "    'max_unix_time'\n",
    "]\n",
    "\n",
    "X_final = final.drop(columns=final_drop_cols)\n",
    "\n",
    "# Label encode if needed\n",
    "final_non_numeric = X_final.select_dtypes(include=['object']).columns\n",
    "for c in final_non_numeric:\n",
    "    X_final[c] = encoder.fit_transform(X_final[c].astype(str))\n",
    "\n",
    "final_probs = rf_model.predict_proba(X_final)[:, 1]\n",
    "final_preds = (final_probs >= optimal_threshold).astype(int)\n",
    "\n",
    "submission = pd.DataFrame({'id': final['id'], 'is_fraud': final_preds})\n",
    "submission.to_csv('submission2.csv', index=False)\n",
    "print(\"Submission file created: submission.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
